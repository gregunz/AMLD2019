{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Curiosity: Intrinsic Curiosity in Machines too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from environments import SUPPORTED_ENVIRONMENTS, make_environment\n",
    "from networks import ActorCritic, IntrinsicCuriosityModule\n",
    "from utils import Recorder, Memory, load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build environments and recorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUPPORTED_ENVIRONMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = make_environment('Pong')\n",
    "breakout = make_environment('Breakout')\n",
    "mario_level_1 = make_environment('SuperMarioBros level 1')\n",
    "mario_level_2 = make_environment('SuperMarioBros level 2')\n",
    "\n",
    "recorder_1 = Recorder()\n",
    "recorder_2 = Recorder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a random agent\n",
    "\n",
    "\n",
    "![RL framework](img/rl.png)\n",
    "_<center>source: Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"</center>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \n",
    "    def play(self, environment, max_games=1, max_steps=500, recorder=None):\n",
    "        \n",
    "        # Reset environment\n",
    "        observation = environment.reset()\n",
    "        \n",
    "        # Initialize infos and recorder\n",
    "        n_games, n_steps = 0, 0\n",
    "        current_game_infos = {'game': 1, 'reward': 0, 'game_duration': 0}\n",
    "        if recorder is not None:\n",
    "            recorder.reset()\n",
    "            recorder.record(environment)\n",
    "\n",
    "        # Main loop\n",
    "        while (n_steps < max_steps) and (n_games < max_games):\n",
    "            \n",
    "            # Interact with environment\n",
    "            # ... = environment.action_space.sample()\n",
    "            # ..., ..., ..., infos = environment.step(...)\n",
    "            \n",
    "            # Update infos and recorder\n",
    "            n_steps += 1\n",
    "            current_game_infos['reward'] += extrinsic_reward\n",
    "            current_game_infos['game_duration'] += 1\n",
    "            if recorder is not None:\n",
    "                recorder.record(environment)\n",
    "            \n",
    "            if is_game_over:\n",
    "                # Update infos\n",
    "                n_games += 1\n",
    "                print(current_game_infos)\n",
    "                current_game_infos = {'game': n_games + 1, 'reward': 0, 'game_duration': 0}\n",
    "                # Reset environment\n",
    "                observation = environment.reset()\n",
    "        \n",
    "        # Stop recorder\n",
    "        if recorder is not None:\n",
    "            recorder.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent()\n",
    "random_agent.play(pong, max_games=1, max_steps=500, recorder=recorder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Smart Agent\n",
    "\n",
    "![Actor Critic architecture](img/actor_critic.png)\n",
    "_<center>source: Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\"</center>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "\n",
    "    def __init__(self, num_actions, checkpoint=None):\n",
    "        \n",
    "        # Initialize network, optimizer and memory\n",
    "        self.network, self.trainable_parameters = self.init_network(num_actions)\n",
    "        self.optimizer = torch.optim.Adam(self.trainable_parameters, lr=1e-4)\n",
    "        self.memory = Memory()\n",
    "        \n",
    "        # Load pretrained model\n",
    "        if checkpoint is not None:\n",
    "            load_checkpoint(self.network, self.optimizer, checkpoint)\n",
    "\n",
    "    def init_network(self, num_actions):\n",
    "        \n",
    "        # Initialize Actor-Critic\n",
    "        network = {'actor_critic': ActorCritic(num_actions)}\n",
    "        trainable_parameters = list(network['actor_critic'].parameters())\n",
    "        return network, trainable_parameters\n",
    "\n",
    "    def play(self, environment, max_games=1, max_steps=500, train=False, verbose=False, recorder=None):\n",
    "        \n",
    "        # Reset environment\n",
    "        observation = environment.reset()\n",
    "        \n",
    "        # Initialize infos and recorder\n",
    "        n_steps = 0\n",
    "        n_games = 0\n",
    "        current_game_infos = {'game': n_games + 1, 'reward': 0, 'game_duration': 0}\n",
    "        if recorder is not None:\n",
    "            recorder.reset()\n",
    "            recorder.record(environment)\n",
    "\n",
    "        # Main loop\n",
    "        while (n_steps < max_steps) and (n_games < max_games):\n",
    "            \n",
    "            # Reset memory\n",
    "            self.init_rollout(observation)\n",
    "            \n",
    "            for rollout_step in range(20):\n",
    "                \n",
    "                # Interact with environment\n",
    "                value, log_policy, action = self.network['actor_critic'](observation)\n",
    "                self.memory.append({'value': value, 'log_policy': log_policy, 'action': action})\n",
    "                \n",
    "                observation, extrinsic_reward, is_game_over, infos = environment.step(action.numpy()[0])\n",
    "                \n",
    "                reward = self.get_reward(observation, extrinsic_reward)\n",
    "                self.memory.append({'reward': reward})\n",
    "\n",
    "                # Update infos and recorder\n",
    "                n_steps += 1\n",
    "                current_game_infos['reward'] += extrinsic_reward\n",
    "                current_game_infos['game_duration'] += 1\n",
    "                if recorder is not None:\n",
    "                    recorder.record(environment)\n",
    "\n",
    "                if is_game_over:\n",
    "                    # Update infos\n",
    "                    n_games += 1\n",
    "                    print(current_game_infos)\n",
    "                    current_game_infos = {'game': n_games + 1, 'reward': 0, 'game_duration': 0}\n",
    "                    # Reset environment\n",
    "                    observation = environment.reset()\n",
    "                    # Interrupt rollout\n",
    "                    break\n",
    "            \n",
    "            self.end_rollout(observation, is_game_over)\n",
    "            if verbose:\n",
    "                print(current_game_infos)\n",
    "            \n",
    "            if train:\n",
    "                # Update neural network\n",
    "                loss = self.compute_loss()\n",
    "                self.backpropagate(loss)\n",
    "\n",
    "        if recorder is not None: recorder.stop()\n",
    "\n",
    "    def init_rollout(self, observation):\n",
    "        self.memory.reset()\n",
    "        self.network['actor_critic'].detach_internal_state()\n",
    "\n",
    "    def end_rollout(self, observation, is_game_over):\n",
    "        if is_game_over:\n",
    "            next_value = torch.Tensor([[0]])\n",
    "            self.network['actor_critic'].reset_internal_state()\n",
    "        else:\n",
    "            next_value = self.network['actor_critic'](observation)[0].detach()\n",
    "        self.memory.append({'value': next_value})\n",
    "\n",
    "    def get_reward(self, observation, extrinsic_reward):\n",
    "        return np.clip(extrinsic_reward, -1, 1)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        return self.network['actor_critic'].loss(self.memory)\n",
    "\n",
    "    def backpropagate(self, loss, max_gradient_norm=40):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.trainable_parameters, max_gradient_norm)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Smart Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_mario_agent = ActorCriticAgent(num_actions=mario_level_1.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_mario_agent.play(mario_level_1, max_games=3, max_steps=500, verbose=True, recorder=recorder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_mario_agent.play(mario_level_1, max_games=10, max_steps=1000000, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_mario_agent.play(mario_level_1, max_games=3, max_steps=500, recorder=recorder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smart_mario_agent = ActorCriticAgent(num_actions=mario_level_1.action_space.n,\n",
    "                                     checkpoint='models/smart_mario_agent_4M.tar')\n",
    "smart_mario_agent.play(mario_level_1, max_games=3, max_steps=500, recorder=recorder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build a Curious Agent\n",
    "\n",
    "![Intrinsic Curiosity Module architecture](img/icm.png)\n",
    "_<center>source: Pathak et al., \"Curiosity driven Exploration by Self-supervision Prediction\" (2017)</center>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuriousActorCriticAgent(ActorCriticAgent):\n",
    "\n",
    "    def init_network(self, num_actions):\n",
    "        network, trainable_parameters = super().init_network(num_actions)\n",
    "        # Initialize Intrinsic Curiosity Module\n",
    "        network['icm'] = IntrinsicCuriosityModule(num_actions)\n",
    "        trainable_parameters += list(network['icm'].parameters())\n",
    "        return network, trainable_parameters\n",
    "\n",
    "    def init_rollout(self, observation):\n",
    "        super().init_rollout(observation)\n",
    "        # Encode the observation into features\n",
    "        features = self.network['icm'].observation_encoder(observation)\n",
    "        self.memory.append({'features': features})\n",
    "\n",
    "    def end_rollout(self, observation, is_game_over):\n",
    "        # Ignore information about the end of the game\n",
    "        next_value = self.network['actor_critic'](observation)[0].detach()\n",
    "        self.memory.append({'value': next_value})\n",
    "\n",
    "    def get_reward(self, observation, extrinsic_reward):\n",
    "        \n",
    "        # Retrieve features and action from the previous step\n",
    "        last_features = self.memory.get_last('features')\n",
    "        last_action = self.memory.get_last('action')\n",
    "        # Encode the observation into features\n",
    "        features = self.network['icm'].observation_encoder(observation)\n",
    "        \n",
    "        # Try to find by yourself the inputs and outputs of these neural networks:\n",
    "        # ... = self.network['icm'].forward_model(..., ...)\n",
    "        # ... = self.network['icm'].inverse_model(..., ...)\n",
    "        \n",
    "        self.memory.append({'features': features,\n",
    "                            'predicted_features': predicted_features,\n",
    "                            'predicted_action': predicted_action})\n",
    "        \n",
    "        # Try to find by yourself the inputs of the curiosity function:\n",
    "        # intrinsic_reward = self.network['icm'].curiosity(..., ...)\n",
    "        return np.clip(intrinsic_reward, -1, 1)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        loss = super().compute_loss()\n",
    "        # Add the ICM loss\n",
    "        loss += self.network['icm'].loss(self.memory)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curious_mario_agent = CuriousActorCriticAgent(num_actions=mario_level_1.action_space.n, \n",
    "                                              checkpoint='models/curious_mario_agent_4M.tar')\n",
    "curious_mario_agent.play(mario_level_1, max_games=5, max_steps=1000, recorder=recorder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_mario_agent = ActorCriticAgent(num_actions=mario_level_1.action_space.n, \n",
    "                                     checkpoint='models/smart_mario_agent_4M.tar')\n",
    "smart_mario_agent.play(mario_level_2, max_games=10, max_steps=5000, recorder=recorder_1, train=True)\n",
    "\n",
    "curious_mario_agent = CuriousActorCriticAgent(num_actions=mario_level_1.action_space.n, \n",
    "                                              checkpoint='models/curious_mario_agent_4M.tar')\n",
    "curious_mario_agent.play(mario_level_2, max_games=10, max_steps=5000, recorder=recorder_2, train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_1.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder_2.replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
